{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Capstone-Project:-Predicting-Daily-Sales-w/-SARIMAX-for-Budget-Customization\" data-toc-modified-id=\"Capstone-Project:-Predicting-Daily-Sales-w/-SARIMAX-for-Budget-Customization-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Capstone Project: Predicting Daily Sales w/ SARIMAX for Budget Customization</a></span></li><li><span><a href=\"#Problem-Statement:\" data-toc-modified-id=\"Problem-Statement:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Problem Statement:</a></span></li><li><span><a href=\"#Executive-Summary:\" data-toc-modified-id=\"Executive-Summary:-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Executive Summary:</a></span></li><li><span><a href=\"#Importing-Packages\" data-toc-modified-id=\"Importing-Packages-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Importing Packages</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualizing-Feature-Distributions\" data-toc-modified-id=\"Visualizing-Feature-Distributions-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Visualizing Feature Distributions</a></span></li><li><span><a href=\"#Exploring-the-Financial-Quarterly-Trends\" data-toc-modified-id=\"Exploring-the-Financial-Quarterly-Trends-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Exploring the Financial Quarterly Trends</a></span><ul class=\"toc-item\"><li><span><a href=\"#Q3-2018----July-1,-2018-to-September-30,-2018\" data-toc-modified-id=\"Q3-2018----July-1,-2018-to-September-30,-2018-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Q3 2018 -- July 1, 2018 to September 30, 2018</a></span></li><li><span><a href=\"#Q4-2018----October-1,-2018-to-December-31,-2018\" data-toc-modified-id=\"Q4-2018----October-1,-2018-to-December-31,-2018-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Q4 2018 -- October 1, 2018 to December 31, 2018</a></span></li><li><span><a href=\"#Q1-2019----January-1,-2019-to-March-31,-2019\" data-toc-modified-id=\"Q1-2019----January-1,-2019-to-March-31,-2019-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Q1 2019 -- January 1, 2019 to March 31, 2019</a></span></li><li><span><a href=\"#Q2-2019----April-1,-2019-to-June-30,-2019\" data-toc-modified-id=\"Q2-2019----April-1,-2019-to-June-30,-2019-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Q2 2019 -- April 1, 2019 to June 30, 2019</a></span></li><li><span><a href=\"#Q3-2019----July-1,-2019-to-September-30,-2019\" data-toc-modified-id=\"Q3-2019----July-1,-2019-to-September-30,-2019-6.2.5\"><span class=\"toc-item-num\">6.2.5&nbsp;&nbsp;</span>Q3 2019 -- July 1, 2019 to September 30, 2019</a></span></li><li><span><a href=\"#Q4-2019----October-1,-2019-to-December-31,-2019\" data-toc-modified-id=\"Q4-2019----October-1,-2019-to-December-31,-2019-6.2.6\"><span class=\"toc-item-num\">6.2.6&nbsp;&nbsp;</span>Q4 2019 -- October 1, 2019 to December 31, 2019</a></span></li></ul></li><li><span><a href=\"#Exploring-Year-Over-Year-(YOY)-Stats\" data-toc-modified-id=\"Exploring-Year-Over-Year-(YOY)-Stats-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Exploring Year Over Year (YOY) Stats</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Q3-2019-Stats-VS-Q3-2018-Stats\" data-toc-modified-id=\"Q3-2019-Stats-VS-Q3-2018-Stats-6.3.0.1\"><span class=\"toc-item-num\">6.3.0.1&nbsp;&nbsp;</span>Q3 2019 Stats VS Q3 2018 Stats</a></span></li><li><span><a href=\"#Q4-2019-Stats-VS-Q4-2018-Stats\" data-toc-modified-id=\"Q4-2019-Stats-VS-Q4-2018-Stats-6.3.0.2\"><span class=\"toc-item-num\">6.3.0.2&nbsp;&nbsp;</span>Q4 2019 Stats VS Q4 2018 Stats</a></span></li></ul></li><li><span><a href=\"#Q2-2019-Stats-VS-Q1-2019-Stats\" data-toc-modified-id=\"Q2-2019-Stats-VS-Q1-2019-Stats-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;</span>Q2 2019 Stats VS Q1 2019 Stats</a></span></li></ul></li><li><span><a href=\"#Daily-Gross-Sales-Correlation-Heatmap\" data-toc-modified-id=\"Daily-Gross-Sales-Correlation-Heatmap-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Daily Gross Sales Correlation Heatmap</a></span></li><li><span><a href=\"#Daily-Gross-Sales-(in-thousands)\" data-toc-modified-id=\"Daily-Gross-Sales-(in-thousands)-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Daily Gross Sales (in thousands)</a></span></li></ul></li><li><span><a href=\"#ADF-Test-for-Stationarity\" data-toc-modified-id=\"ADF-Test-for-Stationarity-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>ADF Test for Stationarity</a></span></li><li><span><a href=\"#ACF-Plot\" data-toc-modified-id=\"ACF-Plot-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>ACF Plot</a></span></li><li><span><a href=\"#PACF-Plot\" data-toc-modified-id=\"PACF-Plot-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>PACF Plot</a></span></li><li><span><a href=\"#Model-Prep\" data-toc-modified-id=\"Model-Prep-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Model Prep</a></span></li><li><span><a href=\"#Model-Selection-with-AIC:\" data-toc-modified-id=\"Model-Selection-with-AIC:-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Model Selection with AIC:</a></span></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Modeling</a></span></li><li><span><a href=\"#Model-Evaluation-In-Sample\" data-toc-modified-id=\"Model-Evaluation-In-Sample-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Model Evaluation In Sample</a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline-Model\" data-toc-modified-id=\"Baseline-Model-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>Baseline Model</a></span></li><li><span><a href=\"#r2-Metric-Score\" data-toc-modified-id=\"r2-Metric-Score-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>r2 Metric Score</a></span></li><li><span><a href=\"#RMSE-Metric-Score\" data-toc-modified-id=\"RMSE-Metric-Score-13.3\"><span class=\"toc-item-num\">13.3&nbsp;&nbsp;</span>RMSE Metric Score</a></span></li><li><span><a href=\"#Residuals-Plot\" data-toc-modified-id=\"Residuals-Plot-13.4\"><span class=\"toc-item-num\">13.4&nbsp;&nbsp;</span>Residuals Plot</a></span></li><li><span><a href=\"#Daily-Gross-Sales-Predictions-SARIMAX-Graph\" data-toc-modified-id=\"Daily-Gross-Sales-Predictions-SARIMAX-Graph-13.5\"><span class=\"toc-item-num\">13.5&nbsp;&nbsp;</span>Daily Gross Sales Predictions SARIMAX Graph</a></span></li></ul></li><li><span><a href=\"#Budget-Calculations\" data-toc-modified-id=\"Budget-Calculations-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Budget Calculations</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Bible:\" data-toc-modified-id=\"The-Bible:-14.1\"><span class=\"toc-item-num\">14.1&nbsp;&nbsp;</span>The Bible:</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Immediate-Budget-Recommendations\" data-toc-modified-id=\"Immediate-Budget-Recommendations-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Immediate Budget Recommendations</a></span></li><li><span><a href=\"#Next-Steps:\" data-toc-modified-id=\"Next-Steps:-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>Next Steps:</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project: Predicting Daily Sales w/ SARIMAX for Budget Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am a consultant hired by restaurant owners from a wine and pizzeria concept, since new management has taken over their costs have gone through the roof and now, they are 25K in the red on average each month. For them to recoup their loss I need to figure out how to properly structure their budgets. I will create a SARIMAX predictive sales model to base my customized budgets from. My metric will be RMSE to gauge my model's performance with because the output will be on the same scale as my data. A successful model will output predictive sales values within a $500.00 range. My model Selection metric will be AIC because of it's ideal use case of smaller data sets such as mine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Procurement; I gathered the data from Touchbistro which is a cloud-based POS (Point of Sale) platform that the company uses. It was exported via 5 separate csv files, that were cleaned then concatenated.The data is from June 1st, 2018 – February 16th, 2020. Quarterly information was only obtained for Q3 & Q4 for 2018 and then all of 2019. Data preproecessing; data was read in as “objects” which then had to be cleaned and converted to their appropriate data types in order to be utilized in my SARIMAX model. SARIMAX models are dependent on a sorted date index with set period frequencies. \n",
    "\n",
    "EDA; I looked at the correlation heatmap of daily sales to get a better sense of what were the best predictors and to visualize any features  that should be dropped because they are too correlated. \"Margin\" and \"Net Sales\" had to be dropped with a positive correlation of 1 percent. Daily Gross Sales are not normally distributed and skewed with what looks like a mean around $4,000.00. \n",
    "Bill Count, Labor Cost, Margin and Net Sales all have similar distributions as Gross Sales which means they are probably highly correlated\n",
    "\n",
    "\n",
    "\n",
    "While exploring quarterly trends I discovered that Daily Gross Sales were stagnant with a growth on average of 1.1 percent when comparing Year over Year Q3 VS Q4 of 2018 and 2019 data. The most interesting observation I noticed was Q1 to Q2 of 2019 the daily gross sales mean increased by $749.00. The quarterly features distributions for the most part tend to follow the same trend as as above, where they are similar to daily gross sales's and are left skewed. \n",
    "\n",
    "These features with similar distributions are also proved to be correlated by visualing the heatmap correaltion graph.Bill count, labor cost and voids tend to have more similar distributions on a quarterly range than on a whole data range. In order to model you must ensure stationarity before modeling. My data has a p-value 0.000280 which gives us evidence to reject the null hypothesis, meaning we accept that our time series is stationary.The ACF and PACF plots showed that the obervations are follow similar trends and are correlated. However ultimately I chose my parameters based on a manual gridsearch of my data. For my model prep I had to split the index manually at the row with 75% of my data for my train set and the other 35% for my test/validation set.Indexes had to be converted to Datetime index with a frequency of “d” in order to be fed into the model. \n",
    "My model's RMSE is off by a mean of around 458.36 which is way better than my baseline model.\n",
    "\n",
    "In conclusion I was able to build a model up to my standards however it was an in-sample model. For this to be applied correctly I will have to go back and look at my out of sample model notebook and figure out how to improve it. As it is completly unusable with a RMSE of 1896 which is no good. I can still look at my Actual Costs vs Actual Sales and offer budget customizations as I did in the functions above. When I do get my out of sample working I can use the same function above to calcuate the custom budgets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "# Import ARIMA model.\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import seaborn as sns\n",
    "# Import Augmented Dickey-Fuller test.\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_reader_cleaner(folder_path):\n",
    "    data_file_csv = [f for f in listdir(folder_path) if isfile(join(folder_path, f)) & f.endswith('.csv')]\n",
    "    df = pd.DataFrame()\n",
    "    for file_path in data_file_csv:\n",
    "        # 1) read in csv\n",
    "        data = pd.read_csv(folder_path + file_path)\n",
    "        # 2) reset_index\n",
    "        data.reset_index(inplace=True)\n",
    "        # 3)set colunms as row 0 \n",
    "        data.columns = data.iloc[0]\n",
    "        # 4) drop first and last columns\n",
    "        data.drop([0, len(data)-1], axis = 0, inplace = True)\n",
    "        # 5) set index as date\n",
    "        data.set_index('Date', inplace = True) \n",
    "        # 6) Print out if nulls\n",
    "        if data.isnull().sum().sum() > 0:\n",
    "            data.isnull().sum().sort_values(ascending = False)         \n",
    "        #  ) concats\n",
    "        df = pd.concat([data,df])\n",
    "    return df\n",
    "merge = data_reader_cleaner('./data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [\"Voids\", \"Gross Sales\", \"Discounts\", \"Net Sales\", \"Menu Item Cost\", \"Labor Cost\", \"Margin\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[t] = merge[t].replace({'\\$': '', ',': ''}, regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[\"Bill Count\"] = merge[\"Bill Count\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge = pd.concat([merge, pd.get_dummies(merge['Day of the Week'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.drop('Day of the Week', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Feature Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Daily Gross Sales are not normally distributed and skewed with what looks like a mean around $4,000.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[\"Gross Sales\"].plot(kind = \"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [\"Voids\", \"Discounts\", \"Net Sales\", \"Menu Item Cost\", \"Labor Cost\", \"Margin\", \"Bill Count\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bill Count, Labor Cost, Margin and Net Sales all have similar distributions as Gross Sales which means they are probably highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[x_cols].hist(figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Financial Quarterly Trends\n",
    "(Below are observations for the entire quarterly section, no need to scroll.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Daily Gross Sales were stagnant with a growth on average of 1.1% when comparing Year over Year Q3 VS Q4 of 2018 and 2019 data.\n",
    "- The most interesting observation I noticed was Q1 to Q2 of 2019 the daily gross sales mean increased by $749.00.\n",
    "- The quarterly features distributions for the most part tend to follow the same trend as as above, where they are similar to daily gross sales's and are left skewed.\n",
    "- These features with similar distributions are also proved to be correlated by visualing the heatmap correaltion graph.\n",
    "Bill count, labor cost and voids tend to have more similar distributions on a quarterly range than on a whole data range. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 2018 -- July 1, 2018 to September 30, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_q_2018 = merge[30:122:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_q_2018.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_q_2018[\"Gross Sales\"].hist(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_q_2018[x_cols].hist(figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_numeric_w_dependent_variable(df, dependent_variable):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    graph = sns.heatmap(df.corr()[[dependent_variable]].sort_values(by=dependent_variable), \n",
    "                    annot=True, \n",
    "                    cmap='coolwarm', \n",
    "                    vmin=-1,\n",
    "                    vmax=1) \n",
    "    plt.title(label = 'Correlation Heatmap of Daily Gross Sales ', fontsize=16)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_numeric_w_dependent_variable(third_q_2018, \"Gross Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4 2018 -- October 1, 2018 to December 31, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_q_2018 = merge[122:212:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_q_2018[\"Gross Sales\"].hist(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_q_2018[x_cols].hist(figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_q_2018.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_numeric_w_dependent_variable(fourth_q_2018, \"Gross Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 2019 -- January 1, 2019 to March 31, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_q_2019 = merge[213:302:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_q_2019.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_q_2019[\"Gross Sales\"].hist(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_q_2019[x_cols].hist(figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_numeric_w_dependent_variable(first_q_2019, \"Gross Sales\")\n",
    "plt.title(label = 'Correlation Heatmap of Daily Gross Sales Q1 2019 ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 2019 -- April 1, 2019 to June 30, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_q_2019 = merge[303:394:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_q_2019.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_q_2019[\"Gross Sales\"].hist(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_q_2019[x_cols].hist(figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_numeric_w_dependent_variable(second_q_2019, \"Gross Sales\")\n",
    "plt.title(label = 'Correlation Heatmap of Daily Gross Sales Q2 2019 ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 2019 -- July 1, 2019 to September 30, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_q_2019 = merge[394:486:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_q_2019.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_q_2019[\"Gross Sales\"].hist(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_q_2019[x_cols].hist( figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heatmap_numeric_w_dependent_variable(third_q_2019, \"Gross Sales\")\n",
    "plt.title(label = 'Correlation Heatmap of Daily Gross Sales Q3 2019 ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4 2019 -- October 1, 2019 to December 31, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_q_2019 = merge[486:574:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_q_2019.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_q_2019[\"Gross Sales\"].hist(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_q_2019[x_cols].hist(figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_numeric_w_dependent_variable(fourth_q_2019, \"Gross Sales\")\n",
    "plt.title(label = 'Correlation Heatmap of Daily Gross Sales Q4 2019 ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Year Over Year (YOY) Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Q3 2019 Stats VS Q3 2018 Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_q_2019.describe() - third_q_2018.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4 2019 Stats VS Q4 2018 Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_q_2019.describe() - fourth_q_2018.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 2019 Stats VS Q1 2019 Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_q_2019.describe() - first_q_2019.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Gross Sales Correlation Heatmap \n",
    "- As expected, the bill count is highly correlated. \n",
    "- This is probably because they have a high turn over rate and low bill average. An area that could be improved.\n",
    "- Saturdays are the highest grossing days, typically that is Friday, so the brunch seems to boost the sales quite a bit. \n",
    "- They might consider opening earlier than 3PM on weekdays to capture the lunch crowd.\n",
    "- Menu Item costs should be more correlated which means they probably are not utilizing this feature correctly. \n",
    "- Discounts and voids should not carry the same correlation. This means they are probably using them interchangeably which is a major issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_numeric_w_dependent_variable(merge, \"Gross Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_corr_cols = [\"Net Sales\", \"Margin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.drop(drop_corr_cols, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Gross Sales (in thousands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(df, cols=None, title='Title', xlab=None, ylab=None, steps=1):\n",
    "    \n",
    "    # Set figure size to be (18, 9).\n",
    "    plt.figure(figsize=(18,9))\n",
    "    \n",
    "    # Iterate through each column name.\n",
    "    for col in cols:\n",
    "            \n",
    "        # Generate a line plot of the column name.\n",
    "        # You only have to specify Y, since our\n",
    "        # index will be a datetime index.\n",
    "        plt.plot(df[col])\n",
    "        \n",
    "    # Generate title and labels.\n",
    "    plt.title(title, fontsize=26)\n",
    "    plt.xlabel(xlab, fontsize=20)\n",
    "    plt.ylabel(ylab, fontsize=20)\n",
    "    \n",
    "    # Enlarge tick marks.\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.xticks(df.index[0::steps], fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(merge, ['Gross Sales'], title = \"Daily Gross Sales(in thousands with quarterly steps)\", steps=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADF Test for Stationarity\n",
    "- 1st step choose Parameter d/D\n",
    "- P-value level of significance is .05\n",
    "- Must ensure stationarity before modeling\n",
    "- My data has a p-value 0.000280 which gives us evidence to reject the null hypothesis, meaning we accept that our time series is stationary.\n",
    "- Parameter d could change after running my manual grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ADF test on original (non-differenced!) data.\n",
    "adfuller(merge['Gross Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code written by Joseph Nelson.\n",
    "def interpret_dftest(dftest):\n",
    "    dfoutput = pd.Series(dftest[0:2], index=['Test Statistic','p-value'])\n",
    "    return dfoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ADF test on original (non-differenced!) data.\n",
    "interpret_dftest(adfuller(merge['Gross Sales']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACF Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2nd step choose Parameter p/P\n",
    "- The autocorrelation plot below shows that the observations are highly correlated with one another.\n",
    "- Observations outside the blue band mean that there is a significant correlation between  Yt and 𝑌𝑡−𝑘 for lag 𝑘\n",
    "- The autoregressive piece to the ARIMA model is responsible for modeling the long-term trends in our time series.\n",
    "- you can tell is follow a trend based in the postive and negative staggering outside of the blue window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot.\n",
    "plot_acf(merge['Gross Sales'].dropna(), lags=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PACF Plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The partial autocorrelation plot checks for the correlation between observations, conditioning on all lower-lag autocorrelations\n",
    "- Observations outside the blue band mean that there is a significant correlation between 𝑌𝑡 and 𝑌𝑡−𝑘for lag 𝑘, accounting for all lower-order lags.\n",
    "- The moving average piece to the ARIMA model is responsible for modeling the short-term trends in our time series\n",
    "- you can tell is follow a trend based in the postive and negative staggering outside of the blue window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(merge['Gross Sales'].dropna(), lags=30);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep\n",
    "I had to split the index manually at the row with 75% of my data for my train set and the other 35% for my test/validation set.\n",
    "Indexes had to be converted to Datetime index with a frequency of “d” in order to be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.index = pd.DatetimeIndex(merge.index).to_period('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "621 *.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_exog = merge.iloc[:217,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_exog = merge.iloc[:404:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_exog.drop(\"Gross Sales\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_exog.drop(\"Gross Sales\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train_exog['Gross Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = X_test_exog['Gross Sales']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection with AIC: \n",
    "\n",
    "I chose AIC because it is typically used for smaller sample sizes over BIC.\n",
    "AIC is for model to model comparison; the selection is based on the smallest AIC  value.\n",
    "I ran a manual grid search to find the best parameters for my model then hardcoded them into my model. \n",
    "\n",
    "\n",
    "These are my best params from my best AIC score :\n",
    "\n",
    "\n",
    "**SARIMAX(1, 1, 1)x(1, 1, 1, 12)12 - AIC:6637.091345311467**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the p, d and q parameters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = SARIMAX(y_train,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=param_seasonal,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            results = mod.fit()\n",
    "\n",
    "            print('SARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima = SARIMAX(endog = y_train,# inout the best params for this model\n",
    "                 order = (1, 1, 1),              # (p, d, q)\n",
    "                 seasonal_order = (1, 1, 1, 12),  # (P, D, Q, S)\n",
    "                 exog = X_train)\n",
    "# Fit SARIMA model\n",
    "model = sarima.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(start = len(y_train), end = len(y_train) + len(y_test) - 1, exog = X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepping index for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.index = y_test.index.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.index = y_train.index.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.index = preds.index.to_timestamp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation In Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[\"Gross Sales\"].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r2 Metric Score\n",
    "- R-squared is a statistical measure of how close the data are to the fitted regression line. Since that is essentially what model is doing under the hood I am going to choose this to interpret it with. 91.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE Metric Score\n",
    "- Root Mean Squared Error Score: 458.36\n",
    "tells you how concentrated the data is around the line of best fit. \n",
    "So my model’s average error of daily sales predications is off by a mean of around 458.36\n",
    "which is way better than my baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test[0:len(y_test)], preds)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'RMSE SCORE {math.sqrt(mse)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can see that the residuals are homoscedastic and bound around +/- 500 since this is on the same scale as my data and my model is predicting daily sales within my desired threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(preds, y_test)\n",
    "plt.title(label = 'Residual Plot of Daily Gross Sales Predictions ', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Gross Sales Predictions SARIMAX Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Follows the same trends as previous data you can see how it follow a similar spike pattern.\n",
    "- Exogenous variables are used for the in-sample predictions which make my  model smarter.\n",
    "- This explains why it was my scores are so good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_train, color = 'blue')\n",
    "plt.plot(y_test, color = 'orange')\n",
    "plt.plot(preds, color = 'green')\n",
    "# plt.plot(future_forecast, color = 'black')\n",
    "\n",
    "plt.title(label = 'Daily Gross Sales Predictions', fontsize=16)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budget Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bible:\n",
    "\n",
    "● Total prime less than 60%\n",
    "\n",
    "● Food cost 25 to 30%\n",
    "\n",
    "● Labor cost BOH 15% to 20%\n",
    "\n",
    "● Labor cost FOH 10 to 15%\n",
    "\n",
    "● Profitmax15%\n",
    "\n",
    "● 7 extreme low. 12% average 15 is excellent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_preds_sales_total = []\n",
    "for i in range(0, len(preds), 7):\n",
    "    chunk = preds[i:i + 7]\n",
    "    weekly_preds_sales_total.append(chunk.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, total in enumerate(weekly_preds_sales_total, start=1):\n",
    "    print(f'Week {i}: Predicted Sales: ${round(total)}')\n",
    "    print(f'Total Prime Cost Less Than: ${round(total*.6,2)}')\n",
    "    print(f'Food Cost Guidelines: Low = ${round(total*.25,2)} to High = ${round(total*.30,2)}')\n",
    "    print(f'Labor Cost BOH Guidelines: Low = ${round(total*.15,2)} to High = ${round(total*.20,2)} ')\n",
    "    print(f'Labor Cost FOH Guidelines: Low = ${round(total*.1,2)} to High = ${round(total*.15,2)}')\n",
    "    print(f'Total Profit Guidelines: Low = ${round(total*.07,2)}, Average = ${round(total*.12,2)} & Excellent = ${round(total*.15,2)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_actual_sales_total = []\n",
    "for i in range(0, len(y_test), 7):\n",
    "    chunk = y_test[i:i + 7]\n",
    "    weekly_actual_sales_total.append(chunk.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, total in enumerate(weekly_actual_sales_total, start=1):\n",
    "\n",
    "    print(f'Week {i}: Actual Sales: ${round(total)}')\n",
    "    \n",
    "    print(f'Total Prime Cost Less Than: ${round(total*.6,2)}')\n",
    "\n",
    "    print(f'Food Cost Guidelines: Low = ${round(total*.25,2)} to High = ${round(total*.30,2)}')\n",
    "\n",
    "    print(f'Labor Cost BOH Guidelines: Low = ${round(total*.15,2)} to High = ${round(total*.20,2)} ')\n",
    "\n",
    "    print(f'Labor Cost FOH Guidelines: Low = ${round(total*.1,2)} to High = ${round(total*.15,2)}')\n",
    "\n",
    "    print(f'Total Profit Guidelines: Low = ${round(total*.07,2)}, Average = ${round(total*.12,2)} & Excellent = ${round(total*.15,2)}')\n",
    "\n",
    "    print() \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "I was able to build a model up to my standards however it was an in-sample model. For this to be applied correctly I will have to go back and look at my out of sample model notebook  and figure out how to improve it. As it is completly unusable with a RMSE of 1896 and r2 of -.15 which is no good. I can still look at my Actual Costs vs Actual Sales and offer budget customizations as I did in the functions above. When I do get my out of sample working I can use the same function above to calcuate the custom budgets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immediate Budget Recommendations\n",
    "Total Prime costs are on average 20% over the suggested budget.\n",
    "The main culprit is BOH labor and food costs.\n",
    "The chef needs to be under weekly budget reviews for his food and labor costs.\n",
    "Cross train back of house staff to work different stations so there are not more staff than needed. \n",
    "The budgets in the mean time will be based on the Previous week’s sales till a proper out of sample model can be provided.\n",
    "- Total prime less than 50%\n",
    "- Food cost 20% MAX\n",
    "- Labor cost BOH 15% MAX\n",
    "- Labor cost FOH 12 % MAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the business they should look at different vendors and request price sheets to compare to current ones. Look into payment plans for vendor accounts to aid in recouping costs.\n",
    "Although BOH labor and food costs seem to be the main issues, if I could gain access to the P & L report it would be  helpful to see where other areas could be improved upon. \n",
    "They might consider opening earlier than 3PM to capture the lunch crowd and launch a happy hour program along with a cocktail menu. If they paid for a liquor license, they should be using it. Beverage programs have a higher profit margin than food and with a properly implemented program it could increase sales up to 5-10 K a week.\n",
    "Look into third party delivery services such as seamless to add a new revenue streams.\n",
    "When I do get my out of sample working I can use the same functions above to calcuate the custom budgets. For the out of sample model Gather more data. Log transform data, I tried this on in sample data for modeling and it was not useful but might be for this model.\n",
    "Try out another model like an RNN that can be utilized on Time Series data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
